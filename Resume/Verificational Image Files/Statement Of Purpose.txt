I enjoy studying computing; see my Github including Putnam Notes and Interviews.

I did nearly all of the Kaggle courses, and had fun spending ~10 hours over 7 sessions on a data science task in R. Then I enrolled in Generalised Linear Models, Scientific Computation In Numerical Analysis, and Statistical Learning/Inference. I contemplated Elements Of Statistical Learning, highly voted Kaggle Finance notebooks, de Prado, and MSCS course text books. I am intrigued by applications of machine learning towards finance. I want to be an Algorithmic Trader or Quantitative Researcher.

I did not take the courses Discrete Math For Computer Science, Introduction To Programming, Data Structures, Algorithms And Complexity, Computer Organisation And Architecture, or Principles Of Computer Systems so I could be interrogated/orally examined via Zoom or MathIM.com e.g.

As for events and experiences which prepared me for graduate study, I have read tens of papers in computer science [mostly from the 70s-90s in enumerative combinatorics and discrete algorithms], and been exposed to non trivial data structures contained therein. I have made insights related to problems, though none thus far have led to publications.

I love a challenge, problem solving, and learning new things and find that tasks on e.g. the Project Euler platform can move me to trawling Stack, ArXiV, Google Scholar, and SciHub.

https://research.ibm.com/haifa/ponderthis/challenges/September2021.html

A story: the September 2021 IBM Research Ponder This of puzzle composer and editor Dr. Gadi Aleksandrowicz. The task was to manipulate a string and produce another string under some constraints such that the "distance" between the 2 strings was under a threshold. Immediately I thought of some canonical results in strings manipulation and in the direction of generalised Hamming distance notions. However, I was not able to alter any of these extant ideas in a meaningful way.

It struck me a couple weeks later that I had missed a true underlying structure. This led to multiple insights, which came one after the other. Firstly, that one might be able to restrict to patterns of the form 0ABC and consider the task as a minimisation of sums of distances in Z3 under the taxicab metric. Secondly, that one can ignore certain input of 1s [fixed cost] to improve performance by significantly reducing the size of this region in Z3 from a 9x9x9 lattice cube to an 8x8x8 lattice cube. Thirdly, that one can, via trial and error, ignore input in order to induce a highly clustered [under this notion/metric of clusteredness] point set in Z3 upon which to execute a 15 dimensional gradient descent to compute the centers of the clusters and produce the target performance rather rapidly. This incurs a low constant term in a first phase to improve performance in a second phase. It was for me one of those tasks which I did not "on sight". Rather, I pondered in the background and had epiphanies! This simpler case, executing this dimensional reduction, imposing a more constraining condition on the used strings, transforms this into an easier task. "This puzzle is one of many examples where asking for a proof of a weaker statement can make the puzzle harder."

This story is about the reality of tricky tasks; going beyond the most simple low latency puzzle solves and asymptotics to more interesting things and optimisations. I followed up with a note, in my Thoughts.tex file, considering the random initiation of gradient descents not from a lattice mesh, but rather from a set which includes points polynomially or exponentially near the vertices and boundary of the convex hull of the search region of inputs for certain objective functions where optima [of desired performance or say a global optimum] are more likely to be found there. One can consider step sizing in these cases as well.

I completed the CSES problem set, learning the most from precisely the tasks which were hardest to me. And I know that this degree will contain quite a few key ideas. I will rise to the long sequence of tasks from which I will emerge more knowledgeable and prepared to apply the science of computers!
